# 爬山演算法
一種簡單的優化算法，此方法就像是在模擬人類爬山時的行為。        
用在單變數，例如 x^2+3x+5
# 隨機下山演算法
x^2+y^2+3x+5y+6 在這種雙變數情形裡面，我們隨機挑一個變數，向左或右移動一小步，只要移動的點更低就接受，直到無法找到最低點。
# 梯度下降法
是一種貪婪演算法，因為她每次都朝著最鞋的方向走去，想要得到最大的下降幅度。  
因為隨機下降法是用隨機的方式，比較沒效率，如果透過微積分計算斜率，就可以總是往較陡峭的那個方向行進，這個微積分概念稱為『梯度』。    
神經網路中的反傳遞演算法就是一種梯度下降法，只是在計算梯度時，改用反傳遞的自動微分，而不是直接以數值方式計算梯度。
## [參考資料](https://misavo.com/blog/%E9%99%B3%E9%8D%BE%E8%AA%A0/%E6%9B%B8%E7%B1%8D/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7/02-%E7%88%AC%E5%B1%B1%E6%BC%94%E7%AE%97%E6%B3%95/A-%E7%88%AC%E5%B1%B1%E6%BC%94%E7%AE%97%E6%B3%95%E7%B0%A1%E4%BB%8B)